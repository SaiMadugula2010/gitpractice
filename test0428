######lambda#######


import boto3
import requests
import os
import json
import zipfile
import tempfile

# Set up Lambda client
client = boto3.client('lambda',
                      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                      aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                      verify=False,   # Remove or set True if needed
                      region_name='us-east-1'  # <-- change to your region
                     )

# Directory to save final ZIPs
final_download_dir = 'final_downloaded_lambdas'
os.makedirs(final_download_dir, exist_ok=True)

# Temporary working directory
working_dir = tempfile.mkdtemp()

# Paginator to get all functions
paginator = client.get_paginator('list_functions')
for page in paginator.paginate():
    for function in page['Functions']:
        function_name = function['FunctionName']
        
        # Filter by name
        if "datalake-mmt" in function_name:
            print(f"Processing {function_name}...")
            
            # Get full function details
            response = client.get_function(FunctionName=function_name)
            
            # 1. Download Lambda code
            download_url = response['Code']['Location']
            r = requests.get(download_url)
            code_zip_path = os.path.join(working_dir, "code.zip")
            with open(code_zip_path, 'wb') as f:
                f.write(r.content)
            
            # 2. Extract code to temp dir
            extracted_code_dir = os.path.join(working_dir, "code")
            os.makedirs(extracted_code_dir, exist_ok=True)
            with zipfile.ZipFile(code_zip_path, 'r') as zip_ref:
                zip_ref.extractall(extracted_code_dir)
            
            # 3. Save configuration JSON
            config_path = os.path.join(working_dir, "config.json")
            with open(config_path, 'w') as f:
                json.dump(response['Configuration'], f, indent=4)
            
            # 4. Create final combined ZIP
            final_zip_path = os.path.join(final_download_dir, f"{function_name}.zip")
            with zipfile.ZipFile(final_zip_path, 'w') as zipf:
                # Add code files
                for foldername, subfolders, filenames in os.walk(extracted_code_dir):
                    for filename in filenames:
                        file_path = os.path.join(foldername, filename)
                        # Preserve folder structure inside 'code/' directory
                        arcname = os.path.relpath(file_path, extracted_code_dir)
                        zipf.write(file_path, arcname=os.path.join("code", arcname))
                
                # Add configuration
                zipf.write(config_path, arcname="config.json")
            
            print(f"Final package created: {final_zip_path}")

print("\n✅ All matching functions downloaded with code + config bundled together.")



#### glue jobs 
import boto3
import requests
import os
import json
import zipfile
import tempfile

# Set up Glue client
client = boto3.client('glue',
                      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                      aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                      verify=False,   # Remove or set True
                      region_name='us-east-1'  # <-- your region
                     )

# Set up S3 client to download scripts
s3 = boto3.client('s3',
                  aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                  aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                  aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                  verify=False,
                  region_name='us-east-1'
                 )

# Final directory to save
final_download_dir = 'final_downloaded_glue_jobs'
os.makedirs(final_download_dir, exist_ok=True)

# Temporary working directory
working_dir = tempfile.mkdtemp()

# List all Glue jobs
paginator = client.get_paginator('get_jobs')
for page in paginator.paginate():
    for job in page['Jobs']:
        job_name = job['Name']
        
        if "datalake-mmt" in job_name:
            print(f"Processing Glue Job: {job_name}...")

            # 1. Get job script location from Command.ScriptLocation
            script_s3_path = job.get('Command', {}).get('ScriptLocation')
            if not script_s3_path:
                print(f"No ScriptLocation found for {job_name}, skipping...")
                continue

            # Parse S3 path
            if script_s3_path.startswith('s3://'):
                s3_parts = script_s3_path.replace('s3://', '').split('/', 1)
                bucket = s3_parts[0]
                key = s3_parts[1]

            # Download the script
            script_path = os.path.join(working_dir, "script.py")
            s3.download_file(bucket, key, script_path)
            
            # 2. Save job configuration
            config_path = os.path.join(working_dir, "config.json")
            with open(config_path, 'w') as f:
                json.dump(job, f, indent=4)
            
            # 3. Create final ZIP
            final_zip_path = os.path.join(final_download_dir, f"{job_name}.zip")
            with zipfile.ZipFile(final_zip_path, 'w') as zipf:
                zipf.write(script_path, arcname="script.py")
                zipf.write(config_path, arcname="config.json")
            
            print(f"Final package created: {final_zip_path}")

print("\n✅ All matching Glue Jobs downloaded with script + config bundled together.")



###### schedules #####
import boto3
import os
import json
import zipfile
import tempfile

# Set up EventBridge Scheduler client
client = boto3.client('scheduler',
                      aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                      aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                      aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                      verify=False,   # Remove or set True
                      region_name='us-east-1'  # <-- your region
                     )

# Final directory to save
final_download_dir = 'final_downloaded_eventbridge_schedules'
os.makedirs(final_download_dir, exist_ok=True)

# Temporary working directory
working_dir = tempfile.mkdtemp()

# Paginator to get all schedules
paginator = client.get_paginator('list_schedules')
for page in paginator.paginate():
    for schedule in page['Schedules']:
        schedule_name = schedule['Name']
        
        if "datalake-mmt" in schedule_name:
            print(f"Processing EventBridge Schedule: {schedule_name}...")

            # Get full schedule details
            schedule_details = client.get_schedule(Name=schedule_name)

            # Save schedule configuration
            config_path = os.path.join(working_dir, "config.json")
            with open(config_path, 'w') as f:
                json.dump(schedule_details, f, indent=4)
            
            # Create final ZIP
            final_zip_path = os.path.join(final_download_dir, f"{schedule_name}.zip")
            with zipfile.ZipFile(final_zip_path, 'w') as zipf:
                zipf.write(config_path, arcname="config.json")
            
            print(f"Final package created: {final_zip_path}")

print("\n✅ All matching EventBridge schedules downloaded with configurations bundled.")











import boto3
import os

# Set up Glue client
glue_client = boto3.client('glue',
                            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                            aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                            verify=False,
                            region_name='us-east-1'  # <-- your region
                          )

# Set up S3 client
s3_client = boto3.client('s3',
                         aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
                         aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
                         aws_session_token=os.getenv('AWS_SESSION_TOKEN'),
                         verify=False,
                         region_name='us-east-1'
                        )

# Directory to save scripts
scripts_dir = 'downloaded_glue_scripts'
os.makedirs(scripts_dir, exist_ok=True)

# List all Glue jobs
paginator = glue_client.get_paginator('get_jobs')
for page in paginator.paginate():
    for job in page['Jobs']:
        job_name = job['Name']
        
        if "datalake-mmt" in job_name:
            print(f"Processing Glue Job: {job_name}...")

            # Get job script location
            script_s3_path = job.get('Command', {}).get('ScriptLocation')
            if not script_s3_path:
                print(f"No ScriptLocation found for {job_name}, skipping...")
                continue

            # Parse S3 path
            if script_s3_path.startswith('s3://'):
                s3_parts = script_s3_path.replace('s3://', '').split('/', 1)
                bucket = s3_parts[0]
                key = s3_parts[1]

                # Set local file path
                script_file_path = os.path.join(scripts_dir, f"{job_name}.py")

                # Download script directly
                s3_client.download_file(bucket, key, script_file_path)

                print(f"✅ Script downloaded: {script_file_path}")
            else:
                print(f"Invalid ScriptLocation format for {job_name}: {script_s3_path}")

print("\n✅ All matching Glue job scripts downloaded.")








requests.exceptions.SSLError: HTTPSConnectionPool(host='prod-04-2014-tasks.s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /snapshots/769656648324/datalake-mmt-material-depth-structure-17b2abd7-4c90-4c9b-9878-cd185719c561?versionId=gmSaOamaIyKjN52LglXqpN3ji65I4.J9&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEN%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIHvwf4gNIsx6Or9gaj1jvTYxl5LVdm6wE8kpHXA9O09eAiAngtq76aBvKe9Llj%2BW0FhPH7RFJPKleC9WyhNLGd7PxyqJAgh3EAAaDDc0OTY3ODkwMjgzOSIM1GInHu9ZzUcEXz9tKuYBiAioeTlmji3KVkBhx6ahaclRAK8zQCe4iAI1HQclu27tzSuK2HNHoJqMMUXyN55shQ2CXom1LM1Iv9h07gOJYSBn5uUh1dJlE3aRr654qd%2FXamMCCi7%2BMfVvqCAXXTEkuAZCocr7OOVlCS9YknBLHbcB%2Fm3gNUemNGz6XbpRiTyP28X3bCYi3a85ywOGoQmSCuPU1Gw2%2B%2B%2B%2BB22P5hzP6nEBH1N6sXqfgTmDxQJw0q2DoSHyGUUYv%2BMABADs3LmX3xvvRepddetg6K1Agc0m1%2FFkf2%2F82H4HBfI55J97sKApVObBFR4wk52%2BwAY6kAFXtZND5iIHLSlz7tC579KNS7izyx9QYN%2FQJZMJmSmG7NXDCh85cPB%2BBq2b4d4N41lSw0zKbYoPh8ZWECUDEFvqcBYdqNsSiLoRiQvUbN6mn8wPzVCgGd5g8KpNb03pGgpqe8Tig0oozfsPmOZXaH46mQ3JOfGgCwg4lK4J7sJ9GgrCnbohlbjp6B8A6OiCtOY%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250428T203945Z&X-Amz-SignedHeaders=host&X-Amz-Expires=600&X-Amz-Credential=ASIA25DCYHY3XEACHWD4%2F20250428%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=abc5149a693a154883fa4968d10d8c4c4da47099a830b1da7ae3e74b0ad59c3f (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:992)')))
