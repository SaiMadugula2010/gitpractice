import numpy as np
import pandas as pd
import pyodbc
import datetime
from openpyxl import Workbook
import smtplib
from email.mime.text import MIMEText

# Database connection string
CONN_STR = "DRIVER={SQL Server Native Client 11.0};SERVER=LPRDDCODM;DATABASE=DCODM01DB;Trusted_Connection=yes;"

# Function to execute stored procedure and return NumPy array
def fetch_data_by_date(rpdate):
    try:
        with pyodbc.connect(CONN_STR) as conn:
            cursor = conn.cursor()
            cursor.execute("EXEC dbo.GetPart39AccountSummaryDataByDate @fromDate = ?, @toDate = ?", (rpdate, rpdate))

            # Fetch results and convert to NumPy array
            columns = [col[0] for col in cursor.description]
            rows = cursor.fetchall()
            if not rows:
                print(f"No data found for {rpdate}")
                return None, None

            data = np.array(rows)
            return data, columns

    except Exception as e:
        print(f"Error fetching data for {rpdate}: {e}")
        return None, None

# Function to get business days of the month
def get_business_days():
    today = datetime.date.today()
    first_day = today.replace(day=1)
    last_month = (first_day - datetime.timedelta(days=1)).replace(day=1)
    business_days = pd.date_range(start=last_month, end=today, freq='B').strftime("%Y-%m-%d").to_numpy()
    return business_days

# Function to process each day's data
def process_days():
    business_days = get_business_days()
    all_data = []

    for date in business_days:
        print(f"Processing data for {date}...")
        data, columns = fetch_data_by_date(date)
        if data is not None:
            all_data.append(data)

    if all_data:
        final_data = np.vstack(all_data)
        return final_data, columns
    return None, None

# Function to calculate top 5 firms using NumPy
def compute_top5(data, col_index):
    unique_dates = np.unique(data[:, 0])  # Assuming first column is 'reportingdate'
    top5_list = []

    for date in unique_dates:
        date_filter = data[:, 0] == date
        segment_data = data[date_filter]
        sorted_indices = np.argsort(segment_data[:, col_index])[::-1]  # Descending sort
        top5_indices = sorted_indices[:5]
        top5_list.append(segment_data[top5_indices])

    return np.vstack(top5_list)

# Function to generate summary report
def generate_report(data):
    unique_dates = np.unique(data[:, 0])  # Assuming first column is 'reportingdate'
    report_summary = []

    for date in unique_dates:
        date_filter = data[:, 0] == date
        daily_data = data[date_filter]

        # Sum relevant columns
        total_futures = np.sum(daily_data[:, 5])  # FO segment
        total_swaps = np.sum(daily_data[:, 6])    # Swaps segment
        total_margin = np.sum(daily_data[:, 7])   # Total margin

        # Get Top 5 for each segment
        top5_fo = compute_top5(daily_data, 5)
        top5_swaps = compute_top5(daily_data, 6)
        top5_total = compute_top5(daily_data, 7)

        # Calculate percentages
        fo_top5_pct = np.sum(top5_fo[:, 5]) / total_futures if total_futures > 0 else 0
        swaps_top5_pct = np.sum(top5_swaps[:, 6]) / total_swaps if total_swaps > 0 else 0
        total_top5_pct = np.sum(top5_total[:, 7]) / total_margin if total_margin > 0 else 0

        # Append summary
        report_summary.append([date, total_futures, total_swaps, total_margin, fo_top5_pct, swaps_top5_pct, total_top5_pct])

    return np.array(report_summary)

# Function to export report to Excel
def save_to_excel(data, columns, filename):
    df = pd.DataFrame(data, columns=columns)
    df.to_excel(filename, index=False)
    print(f"Report saved to {filename}")

# Function to send email notification
def send_email(monthend):
    sender_email = "noreply@cftc.gov"
    recipient_email = "cmarquardt@cftc.gov"
    subject = "***Monthly Cleared Margin Data Updated***"

    body = f"""
    <p>Cleared Margin Data updated for {monthend}.</p>
    <p><a href='\\\\dcprdapsasm02\\CODE_DEV\\CFTC\\DCR\\MonthlyClearedMarginReport'>
    View the report here.</a></p>
    """

    msg = MIMEText(body, "html")
    msg["From"] = sender_email
    msg["To"] = recipient_email
    msg["Subject"] = subject

    with smtplib.SMTP("smtp.cftc.gov") as server:
        server.sendmail(sender_email, recipient_email, msg.as_string())
    print("Email notification sent!")

# Main execution flow
if __name__ == "__main__":
    print("Starting the pipeline...")

    # Fetch and process data
    final_data, columns = process_days()

    if final_data is not None:
        print("Generating report summary...")
        report = generate_report(final_data)

        # Save report
        monthend = datetime.date.today().strftime("%b-%Y")
        output_file = f"CFTC_ClearedMarginReport_{monthend}.xlsx"
        report_columns = ["Date", "Total_Futures", "Total_Swaps", "Total_Margin", "FO_Top5%", "Swaps_Top5%", "Total_Top5%"]
        save_to_excel(report, report_columns, output_file)

        # Send notification
        send_email(monthend)
    else:
        print("No data found for the given period.")





############



##############################################################################################
### Author: Byju Sudhakaran
### Date Started: 4/18/2022
### Last Update Date: 06/22/2023
### Lambda Function Name: mfdc_sdros_file_arrive
###    Language: Python 3.9
##############################################################################################
import json
import re
import uuid
import boto3
import datetime
from datetime import datetime
import os
import logging
import io
import zipfile

#######################################################################
### Global Variables: Set the logger
#######################################################################
logger = logging.getLogger('mfdc_sdros_loader')
formatter = logging.Formatter('[%(levelname)s] %(message)s')
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)
logger.propagate = False
log_level_default = os.environ.get("LOG_LEVEL","DEBUG")
function_name = ""

s3_client = boto3.client('s3')

##############################################################################################
### S3 Event triggers this Lambda Function when a file is dropped in the Raw S3
### bucket.
##############################################################################################
def lambda_handler(event, context): 
    try:
        logger.setLevel(log_level_default)
        logger.debug(f"Event received : {event}")

        function_name = context.function_name

        config_table = verify_and_get_dict_value(function_name, os.environ, 'config_table_name', True)
        app_name = verify_and_get_dict_value(function_name, os.environ, 'app_name', True)

        config_values = setup_config_values_from_dynamoDB(config_table, app_name)

        config_values['file_bucket'] = event['Records'][0]['s3']['bucket']['name']
        config_values['file_key'] = event['Records'][0]['s3']['object']['key']
        key_parts =  config_values['file_key'].split('/')
        config_values['file_name'] = key_parts[-1]
        config_values['processing_file'] = f"s3://{config_values['bucket_info']['staging_bucket']}/{config_values['bucket_info']['staging_key_prefix']}{config_values['file_name'].upper().replace('.ZIP', '.GZ')}"

        logger.debug(f"Processing file {config_values['file_name']} config_table {str(config_table)} app_name {str(app_name)}")
        logger.debug('The config_values is' + str(config_values))
        logger.setLevel(config_values['log'].get("log_level").upper())

        regExCompiled = re.compile(config_values['file_name_regex'])
        matchGroups = regExCompiled.search(config_values['file_name'])
        if matchGroups is not None:
            config_values["file_dt"] = matchGroups.group('file_dt')
            config_values['sdr_id'] = matchGroups.group('sdr_id').upper()
            config_values['asset_class'] = matchGroups.group('asset_class').upper()
            config_values['file_group'] = "SDROS_" + config_values['asset_class'].upper()
        else:
            raise Exception (f"Cannot find sdr_id, file_dt and asset_class from file name: {config_values['file_name']}, Regex: {config_values['file_name_regex']}")

        setup_config_values_for_FileAudit(config_values)

        sf_input = build_sf_input(config_values)
        logger.debug(f"step function input completed {sf_input}. before calling statemachine.")

        sm_response = call_state_machine(config_values, sf_input, config_values['file_name'])
        
    except ValueError as err:
        logger.error('ValueError happened in mfdc_sdros_file_arrive() Lambda Function. The message is: ' + str(err))
        raise err
        
    except Exception as ex:
        template = "An exception of type {0} occurred in " + function_name + ". Arguments:\n{1!r}"
        message = template.format(type(ex).__name__, ex.args)
        logger.error(message)
        raise ex

########################################################################################################################
def setup_config_values_from_dynamoDB(config_table, app_name):
    config_values = None
    response_payload = None
    logger.debug(f"Reading from DynamoDB: config_table - {config_table} app_name - {app_name}")

    dynamoDB = boto3.resource('dynamodb',region_name='us-east-1')
    config_table = dynamoDB.Table(config_table)
    response_payload = config_table.get_item(Key={'application_name': app_name})
    logger.debug("Response from Dynamo: {}".format(response_payload))
    config_values = response_payload['Item']
    config_values['app_name'] = app_name
    
    return config_values

########################################################################################################################
def setup_config_values_for_FileAudit(config_values):
    file_audit = {}
    log_parms = {}
    file_audit["file_path"] =  f"{config_values['file_bucket']}/{config_values['file_key'].rsplit('/',1)[0]}"
    file_audit["file_name"] =  config_values['file_key'].rsplit('/',1)[-1]
    file_audit["process_step"] = "STAGE"
    file_audit["data_stream"] = "SDR"
    file_audit["sub_data_stream"] = f"{config_values['sdr_id']}_{config_values['asset_class']}"
    file_audit["action"] = "insert"
    file_audit['loader'] = "SDR 2.0"
    log_parms["file_audit"] = file_audit
    config_values["LogParams"] = log_parms
    config_values['ParentFileId'] = getAuditId(config_values["LogParams"])

    fileElements = config_values['processing_file'].rsplit('/',1)
    file_audit["file_name"] =  fileElements[1]
    file_audit["file_path"] = fileElements[0]
    file_audit["parent_file_id"] = config_values['ParentFileId']
    log_parms["file_audit"] = file_audit
    config_values["LogParams"] = log_parms
    print(config_values["LogParams"])
    config_values['auditID'] = getAuditId(config_values["LogParams"])
    
    return config_values

########################################################################################################################
def getAuditId(fileAuditConfig):
    logger.debug(f"fileAuditConfig - {fileAuditConfig}")
    lambdaWraper = boto3.client('lambda')
    lambdaPayload = fileAuditConfig
    logger.debug(f"lambdaPayload - {json.dumps(lambdaPayload)}")
    response = lambdaWraper.invoke(FunctionName='mfdc_file_audit', InvocationType='RequestResponse',  Payload=json.dumps(lambdaPayload))
    logger.debug(f"response {response}")
    fileId = ""
    if 'Payload' in response:
        response = json.loads(response["Payload"].read().decode("utf-8"))
        fileId = response["file_id"]
    else:
        raise ValueError("Unable to generate FileID. Cannot continue file load.")

    return fileId

########################################################################################################################
def verify_and_get_dict_value(function_name, dict_of_values, data_key, is_required_field):
    value_error = False
    value = ''
    errorMsg = ''

    if data_key in dict_of_values:
        value = dict_of_values[data_key]
        if (value is None or len(value) == 0) and is_required_field:
            errorMsg = f"ValueError happened in {function_name}. No data was found for Key: {data_key} in the dictionary."
            value_error = True
    else:
        if is_required_field:
            errorMsg = f"Key {data_key} not found in the dictionary."
            value_error = True

    if value_error and is_required_field:
        logger.debug(errorMsg)
        raise ValueError(errorMsg)

    return value

########################################################################################################################
def build_sf_input(config_values):
    logger.debug(f"Config values - {config_values}")
    source_file = f"{config_values['file_bucket']}/{config_values['file_key']}"
    sf_values = {}

    sf_values["SourceBucket"] = config_values['file_bucket']
    sf_values["SourceKey"] = config_values['file_key']
    sf_values["SourceBucketWithKey"] = source_file
    sf_values["StagingBucket"] = config_values["bucket_info"]["staging_bucket"]
    sf_values["StagingKey"] = f"{config_values['bucket_info']['staging_key_prefix']}{config_values['file_name'].upper().replace('.zip', '.gz')}"
    sf_values["ArchiveBucket"] = config_values["bucket_info"]["archive_bucket"]
    sf_values["ArchiveKey"] = f"{config_values['bucket_info']['archive_key_prefix']}{config_values['sdr_id']}/{config_values['file_name']}"

    sf_values["LogLevel"] = config_values['log'].get("log_level").upper()
    sf_values["FileName"] = config_values['file_name']
    sf_values["HeaderBuffer"] = config_values['header_buffer']

    sf_values["LambdaFunctions"] = {
        "BuildSnsMsg": config_values["lambda_functions"]["build_sns_msg"],
        "ExecuteRsCommand": config_values["lambda_functions"]["execute_rs_command"],
        "SendTaskResult": config_values["lambda_functions"]["send_task_result"],
        "ConvertToJson": config_values["lambda_functions"]["convert_to_json"],
        "LogAudit": config_values["lambda_functions"]["file_audit"],
        "RecCheckLambda": config_values["lambda_functions"]["rec_check_lambda"],
        "CreateRedshiftQueries": config_values["lambda_functions"]["create_redshift_queries"]
    }

    sf_values["SuccessBody"] = config_values["notifications"]["success"]["body"]
    sf_values["SuccessSubject"] = config_values["notifications"]["success"]["subject"]
    sf_values["FailureBody"] = config_values["notifications"]["failure"]["body"]
    sf_values["FailureSubject"] = config_values["notifications"]["failure"]["subject"]
    sf_values["NotificationArn"] = config_values["notifications"]["notification_arn"]

    sf_values['Batch'] = {
        "JobQueue": config_values["batch"]["job_queue"],
        "JobDefinition": config_values["batch"]["job_definition"],
        "SourceFile": f"s3://{source_file}",
        "OutputFile": config_values['processing_file'],
        "AuditId": config_values["auditID"]
    }

    sf_values['LogParams'] = config_values["LogParams"]
    sf_values["FileDt"] = config_values["file_dt"]
    sf_values["SdrId"] = config_values['sdr_id']
    sf_values["AssetClass"] = config_values['asset_class']
    sf_values["FileGroup"] = config_values['file_group']

    logger.debug("Step Function Input: {}".format(sf_values))
    return sf_values

########################################################################################################################
def call_state_machine(config_values, sm_input, file_name):
    loader_arn = config_values["step_functions"]["loader_arn"]
    current_time_str = datetime.now().strftime("%H_%M_%S")
    sm_name = file_name + '_' + current_time_str
    sm_name = sm_name.replace('.', '_')

    logger.debug('The sm_name is : ' + str(sm_name))
    stepFun_client = boto3.client("stepfunctions")
    logger.debug('Right BEFORE calling the State Machine with the ARN of: ' + str(loader_arn))

    response = stepFun_client.start_execution(
        stateMachineArn=loader_arn,
        name=sm_name,
        input=json.dumps(sm_input)
    )

    logger.debug('Right AFTER calling the State Machine')
    response = json.loads(json.dumps(response, default=str))
    logger.debug("State Machine Response was: {}".format(response))
    return response



