##############################################################################################
### Author: Byju Sudhakaran
### Date Started: 4/18/2022
### Last Update Date: 06/22/2023
### Lambda Function Name: mfdc_sdros_file_arrive
###    Language: Python 3.9
##############################################################################################
import json
import re
import uuid
import boto3
import datetime
from datetime import datetime
import os
import logging
import io
import zipfile

#######################################################################
### Global Variables: Set the logger
#######################################################################
logger = logging.getLogger('mfdc_sdros_loader')
formatter = logging.Formatter('[%(levelname)s] %(message)s')
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)
logger.propagate = False
log_level_default = os.environ.get("LOG_LEVEL","DEBUG")
function_name = ""

s3_client = boto3.client('s3')

##############################################################################################
### S3 Event triggers this Lambda Function when a file is dropped in the Raw S3
### bucket.
##############################################################################################
def lambda_handler(event, context): 
    try:
        logger.setLevel(log_level_default) #set initial log level, will be overriden, needed for pre-config grab from db
        logger.debug(f"Event received : {event}") #log the arrival event

        ##################################################################
        ### Get the name of the Lambda Function from the context object.
        ### It will be used if this lambda function throws exception.
        ##################################################################
        function_name = context.function_name

        ##################################################################
        ### Get the environment variables.
        ##################################################################
        config_table = verify_and_get_dict_value(function_name, os.environ, 'config_table_name', True);
        app_name = verify_and_get_dict_value(function_name, os.environ, 'app_name', True);

        #Get config entries from DynamoDB database.
        config_values = setup_config_values_from_dynamoDB(config_table, app_name);

        config_values['file_bucket'] = event['Records'][0]['s3']['bucket']['name'];
        config_values['file_key'] = event['Records'][0]['s3']['object']['key'];
        #extract file_name from the key. Save file_name to config_values
        key_parts =  config_values['file_key'].split('/')
        config_values['file_name'] = key_parts[-1]
        config_values['processing_file'] = f"s3://{config_values['bucket_info']['staging_bucket']}/{config_values['bucket_info']['staging_key_prefix']}{config_values['file_name'].upper().replace('.ZIP', '.GZ')}"

        logger.debug(f"Processing file {config_values['file_name']} config_table {str(config_table)} app_name {str(app_name)}  ") #log file name
        logger.debug('The config_values is' + str(config_values) )
        logger.setLevel(config_values['log'].get("log_level").upper())
        
        #Extract SDRID, asset class and Date from file name
        regExCompiled = re.compile(config_values['file_name_regex'])
        matchGroups = regExCompiled.search(config_values['file_name'] )
        if matchGroups is not None:
            config_values["file_dt"] = matchGroups.group('file_dt')
            config_values['sdr_id'] = matchGroups.group('sdr_id').upper()
            config_values['asset_class'] = matchGroups.group('asset_class').upper()
            config_values['file_group'] = "SDROS_" + config_values['asset_class'].upper()
        else:
            raise Exception (f"Cannot find sdr_id, file_dt and asset_class from file name: {config_values['file_name']}, Regex: {config_values['file_name_regex']}")

        #initialize config values for file audit
        setup_config_values_for_FileAudit(config_values)

        #Build step function input
        sf_input = build_sf_input(config_values)
        logger.debug(f"step function input completed {sf_input}. before calling statemachine." )
        #Call statemachine    
        sm_response = call_state_machine(config_values, sf_input, config_values['file_name'] );
            
    except ValueError as err:
        logger.error('ValueError happened in mfdc_sdros_file_arrive() Lambda Function. The message is: ' + str(err));
        raise err
        
    except Exception as ex:
        template = "An exception of type {0} occurred in " + function_name + ". Arguments:\n{1!r}"
        message = template.format(type(ex).__name__, ex.args)
        logger.error(message);
        raise ex 

########################################################################################################################
### Function: Builds the step function input json.
### Collects all config values and builds a dictionary with all the relevant info to be passed to the step function.
########################################################################################################################
def build_sf_input(config_values):
    logger.debug(f"Config values - {config_values}")
    source_file = f"{config_values['file_bucket']}/{config_values['file_key']}"

    # Create dynamic folder path based on sdr_id
    sdr_id_folder = config_values['sdr_id'].upper()  # New: Create folder name from sdr_id
    archive_key_prefix = config_values['bucket_info']['archive_key_prefix'].replace('{sdr_id}', sdr_id_folder)

    sf_values = {}

    sf_values["SourceBucket"] = config_values['file_bucket']
    sf_values["SourceKey"] = config_values['file_key']
    sf_values["SourceBucketWithKey"] = source_file 
    sf_values["StagingBucket"] = config_values["bucket_info"]["staging_bucket"]
    sf_values["StagingKey"] = f"{config_values['bucket_info']['staging_key_prefix']}{config_values['file_name'].upper().replace('.zip', '.gz')}"
    sf_values["ArchiveBucket"] = config_values["bucket_info"]["archive_bucket"]
    sf_values["ArchiveKey"] = f"{archive_key_prefix}{config_values['file_name']}"  # New: Archive in subfolder based on sdr_id

    sf_values["LogLevel"] = config_values['log'].get("log_level").upper()
    sf_values["FileName"] = config_values['file_name']

    sf_values["HeaderBuffer"] = config_values['header_buffer']

    sf_values["LambdaFunctions"] = {
        "BuildSnsMsg": config_values["lambda_functions"]["build_sns_msg"],
        "ExecuteRsCommand": config_values["lambda_functions"]["execute_rs_command"],
        "SendTaskResult": config_values["lambda_functions"]["send_task_result"],
        "ConvertToJson": config_values["lambda_functions"]["convert_to_json"],
        "LogAudit": config_values["lambda_functions"]["file_audit"],
        "RecCheckLambda": config_values["lambda_functions"]["rec_check_lambda"],
        "CreateRedshiftQueries": config_values["lambda_functions"]["create_redshift_queries"]
    }

    sf_values["SuccessBody"] = config_values["notifications"]["success"]["body"]
    sf_values["SuccessSubject"] = config_values["notifications"]["success"]["subject"]
    sf_values["FailureBody"] = config_values["notifications"]["failure"]["body"]
    sf_values["FailureSubject"] = config_values["notifications"]["failure"]["subject"]
    sf_values["NotificationArn"] = config_values["notifications"]["notification_arn"]

    sf_values['Batch'] = {
        "JobQueue" : config_values["batch"]["job_queue"],
        "JobDefinition" : config_values["batch"]["job_definition"],
        "SourceFile" : f"s3://{source_file}",
        "OutputFile" : config_values['processing_file'],
        "AuditId" : config_values["auditID"] 
    }

    sf_values['LogParams'] = config_values["LogParams"]

    sf_values["FileDt"] = config_values["file_dt"]
    sf_values["SdrId"] = config_values['sdr_id']
    sf_values["AssetClass"] = config_values['asset_class']
    sf_values["FileGroup"] = config_values['file_group']

    logger.debug("Step Function Input: {}" + str(sf_values))

    return sf_values

########################################################################################################################
### Function: Call the State Machine 
########################################################################################################################
def call_state_machine(config_values, sm_input, file_name): 
    loader_arn = config_values["step_functions"]["loader_arn"]

    current_time_str = datetime.now().strftime("%H_%M_%S")
    sm_name = file_name + '_' + current_time_str
    sm_name = sm_name.replace('.', '_')

    logger.debug('The sm_name is : ' + str(sm_name))

    stepFun_client = boto3.client("stepfunctions")

    logger.debug('Right BEFORE calling the State Machine with the ARN of: ' + str(loader_arn))

    response = stepFun_client.start_execution(stateMachineArn=loader_arn,
                    name=sm_name,
                    input=json.dumps(sm_input))

    logger.debug('Right AFTER calling the State Machine')

    #Sanitize the statemachine response to turn the dates into strings
    response = json.loads(json.dumps(response,default=str))
    logger.debug("State Machine Response was: {}".format(response))

    return response
